# System Architecture

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Interface Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  CLI App     â”‚  â”‚  Gradio UI   â”‚  â”‚   API        â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Agentic Control Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           MultimodalRAGAgent                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚Query       â”‚  â”‚Planning  â”‚  â”‚Synthesis     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚Analysis    â”‚  â”‚          â”‚  â”‚              â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Retrieval    â”‚  â”‚   Analysis   â”‚  â”‚   Reasoning   â”‚
â”‚     Tools      â”‚  â”‚    Tools     â”‚  â”‚     Tools     â”‚
â”‚                â”‚  â”‚              â”‚  â”‚               â”‚
â”‚ â€¢ Multimodal   â”‚  â”‚ â€¢ Visual     â”‚  â”‚ â€¢ Temporal    â”‚
â”‚   Retriever    â”‚  â”‚   Analyzer   â”‚  â”‚   Retriever   â”‚
â”‚ â€¢ Text Search  â”‚  â”‚ â€¢ VLM        â”‚  â”‚ â€¢ Cross-Modal â”‚
â”‚ â€¢ Visual Searchâ”‚  â”‚   Analysis   â”‚  â”‚   Linker      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Storage & Inference Layer                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Qdrant     â”‚  â”‚   Ollama    â”‚  â”‚  Embedding   â”‚ â”‚
â”‚  â”‚  (Vectors)   â”‚  â”‚   (LLMs)    â”‚  â”‚   Models     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Frames    â”‚  â”‚  Slides    â”‚  â”‚ Transcriptsâ”‚      â”‚
â”‚  â”‚  (Images)  â”‚  â”‚  (OCR)     â”‚  â”‚  (Text)    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Data Flow

### 1. Preprocessing Pipeline

```
Video File
    â”‚
    â”œâ”€â–º VideoExtractor â”€â”€â”€â”€â”€â–º Frames (1 FPS)
    â”‚                          â”‚
    â”‚                          â”œâ”€â–º CLIP Embeddings
    â”‚                          â””â”€â–º Frame Metadata
    â”‚
    â”œâ”€â–º VideoExtractor â”€â”€â”€â”€â”€â–º Audio (WAV)
    â”‚                          â”‚
    â”‚                          â””â”€â–º Whisper â”€â”€â”€â”€â”€â–º Transcript + Timestamps
    â”‚                                             â”‚
    â”‚                                             â””â”€â–º Text Chunks
    â”‚
    â””â”€â–º SlideDetector â”€â”€â”€â”€â”€â”€â–º Unique Slides
                               â”‚
                               â”œâ”€â–º OCR â”€â”€â”€â”€â”€â–º Text Content
                               â””â”€â–º CLIP â”€â”€â”€â”€â–º Visual Embeddings
```

### 2. Indexing Pipeline

```
Processed Data
    â”‚
    â”œâ”€â–º Text Chunks â”€â”€â”€â”€â–º TextEmbedder â”€â”€â”€â”€â–º Qdrant (text_collection)
    â”‚
    â”œâ”€â–º Frames â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º CLIPEmbedder â”€â”€â”€â”€â–º Qdrant (visual_collection)
    â”‚
    â””â”€â–º Slides â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â–º CLIPEmbedder â”€â”€â”
                        â””â”€â–º TextEmbedder â”€â”€â”´â”€â–º Qdrant (slide_collection)
                                                [Multimodal: visual + text]
```

### 3. Query Processing Pipeline

```
User Query
    â”‚
    â””â”€â–º 1. Query Analysis (LLM)
         â”‚  - Detect modalities needed
         â”‚  - Identify temporal requirements
         â”‚  - Determine if VLM needed
         â”‚
         â””â”€â–º 2. Retrieval Planning
              â”‚
              â”œâ”€â–º Text Retrieval
              â”‚    â””â”€â–º Vector Search (text embeddings)
              â”‚
              â”œâ”€â–º Visual Retrieval
              â”‚    â””â”€â–º Vector Search (CLIP embeddings)
              â”‚
              â””â”€â–º Slide Retrieval
                   â””â”€â–º Hybrid Search (visual + text)
              â”‚
              â””â”€â–º 3. Enhancement
                   â”‚
                   â”œâ”€â–º Visual Analysis (if needed)
                   â”‚    â””â”€â–º VisionLLM analyzes frames
                   â”‚
                   â”œâ”€â–º Temporal Reasoning (if needed)
                   â”‚    â””â”€â–º Sort by timestamp, find progression
                   â”‚
                   â””â”€â–º Cross-Modal Linking (if needed)
                        â””â”€â–º Link text â†” visual content
              â”‚
              â””â”€â–º 4. Synthesis
                   â”‚
                   â””â”€â–º LLM combines all sources
                        â””â”€â–º Final Answer + Citations
```

## ğŸ§© Component Details

### Vector Store Collections

#### 1. Text Collection
- **Vectors**: Text embeddings (768D - nomic-embed-text)
- **Metadata**:
  - `lecture_id`: str
  - `timestamp`: float
  - `timestamp_end`: float
  - `content`: str (actual text)
  - `word_count`: int

#### 2. Visual Collection
- **Vectors**: CLIP embeddings (512D)
- **Metadata**:
  - `lecture_id`: str
  - `timestamp`: float
  - `frame_path`: str
  - `frame_id`: int
  - `is_slide`: bool

#### 3. Slide Collection
- **Vectors**: Named vectors
  - `visual`: CLIP embedding (512D)
  - `text`: Text embedding (768D)
- **Metadata**:
  - `lecture_id`: str
  - `timestamp`: float
  - `timestamp_end`: float
  - `slide_id`: int
  - `slide_path`: str
  - `ocr_text`: str
  - `has_diagram`: bool
  - `has_code`: bool

### Agent Decision Flow

```python
# Simplified pseudocode of agent logic

def query(user_query):
    # Step 1: Analyze
    analysis = llm.analyze_query(user_query)
    # Returns: {modalities, temporal, visual_analysis, cross_modal}
    
    # Step 2: Retrieve
    results = {}
    if 'text' in analysis.modalities:
        results['text'] = retrieve_text(user_query)
    if 'visual' in analysis.modalities:
        results['visual'] = retrieve_visual(user_query)
    if 'slides' in analysis.modalities:
        results['slides'] = retrieve_slides(user_query)
    
    # Step 3: Enhance
    if analysis.visual_analysis:
        for visual_result in results['visual'][:2]:
            frame_path = visual_result.frame_path
            analysis = vision_llm.analyze(frame_path, user_query)
            results['visual_analysis'].append(analysis)
    
    if analysis.temporal:
        results = sort_by_timestamp(results)
        results['progression'] = identify_concept_flow(results)
    
    if analysis.cross_modal:
        for text_result in results['text']:
            linked_visual = find_visual_at_same_time(text_result)
            results['links'].append({text_result, linked_visual})
    
    # Step 4: Synthesize
    answer = llm.synthesize(
        query=user_query,
        text=results['text'],
        visual=results['visual'],
        analysis=results['visual_analysis'],
        links=results['links']
    )
    
    return answer
```

## ğŸ“Š Model Specifications

| Model | Purpose | Size | Device | Speed |
|-------|---------|------|--------|-------|
| **nomic-embed-text-v1.5** | Text embeddings | 137M params | GPU/CPU | ~1000 docs/sec |
| **CLIP ViT-L/14** | Vision embeddings | 428M params | GPU | ~100 imgs/sec |
| **Whisper Large-v3** | Transcription | 1.5B params | GPU | ~10x realtime |
| **Llama 3.1 8B** | Main reasoning | 8B params | GPU | ~20 tokens/sec |
| **Llama 3.2 Vision 11B** | Frame analysis | 11B params | GPU | ~15 tokens/sec |

## ğŸ¯ Key Design Decisions

### 1. Why Separate Collections?
- **Performance**: Optimized index per modality
- **Flexibility**: Different embedding dimensions
- **Scalability**: Can scale collections independently

### 2. Why Temporal Metadata?
- Enables "before/after" queries
- Supports concept progression tracking
- Allows temporal filtering

### 3. Why Agentic Approach?
- **Not all queries need all modalities**: Save compute
- **Dynamic tool selection**: Better accuracy
- **Explainability**: Show reasoning steps

### 4. Why Local Models?
- **Privacy**: No data leaves your machine
- **Cost**: No API fees
- **Control**: Customize models as needed

## ğŸ” Retrieval Strategies

### Hybrid Search for Slides
```python
# Slides have BOTH visual and text embeddings
# Can search by either modality

# Text-based search (OCR content)
results = search_slides(query_embedding, mode="text")

# Visual-based search (diagram/image)
results = search_slides(query_embedding, mode="visual")
```

### Temporal Window Retrieval
```python
# Get content around a timestamp
context = get_temporal_context(
    lecture_id="lecture_03",
    timestamp=245.3,  # Result timestamp
    window=30  # Â±30 seconds
)
# Returns text spoken and slides shown in that window
```

### Cross-Modal Linking
```python
# Find what was SHOWN when something was SAID
text_result = search_text("Q-learning update rule")
visual_result = link_to_visual(text_result)
# Returns slide shown at that timestamp
```

## ğŸš€ Performance Optimizations

1. **Batch Embedding**: Process images in batches of 32
2. **Caching**: Cache frequently accessed embeddings
3. **Lazy Loading**: Only analyze frames when needed
4. **Quantization**: Use FP16 for faster inference
5. **Frame Sampling**: 1 FPS instead of full frame rate

## ğŸ”® Future Enhancements

1. **Concept Knowledge Graph**: Build graph of lecture concepts
2. **Reranking**: Use cross-encoder for better ranking
3. **Multi-Lecture Search**: Search across entire course
4. **Auto Quiz Generation**: Generate questions from content
5. **Personalized Learning**: Track user progress
6. **Real-time Processing**: Process videos as they stream
